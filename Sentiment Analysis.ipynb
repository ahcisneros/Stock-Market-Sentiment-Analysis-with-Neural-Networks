{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.layers import Conv1D, Flatten, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from keras.layers import Dropout\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Alexis\\Documents\\Data_Scraping\\data\\{}.csv'.format(\"twitter_sentiment_1\"), encoding=\"Latin-1\")\n",
    "df['target'] = df[\"target\"].replace(4, 1) # replace labels of 4 with 1, easier for the computer to compute\n",
    "df.drop([\"ids\",\" flag\",\" user\"],axis=1,inplace=True) # drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def cleaning(data):\n",
    "    porter = PorterStemmer()\n",
    "    lancaster= LancasterStemmer()\n",
    "    # calibrated stop word list\n",
    "    updated_stopwords = ['i', 'me', 'my', 'myself', 'we', \"weve\", \"wev\", 'our', 'ours', 'ourselves', 'you', \n",
    "                         \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "                         'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", \n",
    "                         'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', \n",
    "                         'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "                         'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "                         'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
    "                         'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "                         'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "                         'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                         'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than', \n",
    "                         'too', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd',\n",
    "                         'll', 'm', 'o', 're', 've', 'y', 'ma', \"youre\", \"youve\", \"youll\", \"youd\", \n",
    "                         \"shes\", \"its\", \"thatll\", \"hes\", \"im\", \"x\", \"well\", \"wel\", \"w\", \"i'm\", \"u\", \"b\", \"theyre\", \"they're\",\n",
    "                         \"ms\", \"mrs\", \"mr\", \"s\"]\n",
    "    \n",
    "    text_df = data[\" text\"]\n",
    "    list_1 = []  \n",
    "    for text in text_df:\n",
    "        html_decoded = BeautifulSoup(text, \"lxml\")# html decoding\n",
    "        html_decoded = html_decoded.get_text() # gets the text from the decoded html\n",
    "        \n",
    "        try: \n",
    "            html_decoded = html_decoded.decode(\"utf-8-sig\") # decoded using utf-8-sig\n",
    "            no_utf = html_decoded.replace(u\"\\ufffd\", \" \") # removes UTF-8 BOM\n",
    "        except: #if nothing is found then the make the html_decoded equal no_utf without going through the utf\n",
    "            no_utf = html_decoded\n",
    "        no_utf = no_utf.encode('utf8').decode('utf8')\n",
    "        string_no_tags = re.sub(r\"@[A-Za-z0-9-_]+\",\" \", no_utf) # removes tags\n",
    "        \n",
    "        no_urls = re.sub(r\"http\\S+\", \" \", string_no_tags) # remove urls\n",
    "       \n",
    "        no_numbers = re.sub(r\"\\d+\", \" \", no_urls) # remove numbers \n",
    "        \n",
    "        no_hashtags = re.sub(r\"#[A-Za-z0-9]+\",\" \",no_numbers) # removes hashtags and anything following it\n",
    "        no_hashtags = [word.lower() for word in no_hashtags.split()]\n",
    "    \n",
    "        removed_stopwords = [word for word in no_hashtags if word not in updated_stopwords]\n",
    "        \n",
    "        stem = \"\"\n",
    "        for words in removed_stopwords:\n",
    "            stem += \"{} \".format(porter.stem(words))\n",
    "            \n",
    "        duplicates_deleted = re.sub('\\s+', ' ', stem).strip() # all spaces are converted to a single spac\n",
    "  \n",
    "        # removes duplicate characters\n",
    "        duplicates_deleted = \"\"\n",
    "        check = \"\"\n",
    "        counter = 0\n",
    "        for word in removed_stopwords:\n",
    "            for character in word:\n",
    "                counter += 1\n",
    "                if character != check:\n",
    "                    check = character\n",
    "                    duplicates_deleted += \"{}\".format(character)\n",
    "                else:\n",
    "                    continue \n",
    "            if len(word) == counter: \n",
    "                duplicates_deleted += \"{}\".format(\" \")\n",
    "                counter = 0\n",
    "                check = \"\"\n",
    "        duplicates_deleted = duplicates_deleted.replace(\".\", \" \")\n",
    "            \n",
    "        no_punctuation = \"\".join([char.lower() for char in duplicates_deleted if char not in string.punctuation]) # remove punctuation\n",
    "        tweets = no_punctuation.lower() # turn everything lowercase\n",
    "        #print(tweets)\n",
    "        tweets_1 = \"\"\n",
    "        for words in tweets.split():\n",
    "            tweets_1 += \"{} \".format(porter.stem(words))\n",
    "        \n",
    "        tweets = re.sub('\\s+', ' ', tweets_1).strip() # all spaces are converted to a single spac\n",
    "        list_1.append(tweets) # add each list to tweet  \n",
    "        #print(\"filtered & cleaned tweets: \" + tweets + \"\\n\")\n",
    "        \n",
    "    dict_1 = {\"text\": list_1}  # create dictionary for dataframe structure\n",
    "    cleaned_dataframe = pd.DataFrame(dict_1) # creates the dataframe\n",
    "    return cleaned_dataframe\n",
    "\n",
    "cleaned_df = cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(cleaned_df, old_df):\n",
    "    targetList = []\n",
    "    dateList = []\n",
    "    textList = []\n",
    "    for f in old_df[\" date\"]:\n",
    "        dateList.append(f)\n",
    "    for f in old_df[\"target\"]:\n",
    "        targetList.append(f)\n",
    "    for f in cleaned_df[\"text\"]:\n",
    "        textList.append(f)\n",
    "    \n",
    "    merged_df = pd.DataFrame({\"date\": dateList, \"target\": targetList,\"text\": textList})\n",
    "    return merged_df   \n",
    "\n",
    "merged_df = combine(cleaned_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in merged_df.iterrows():\n",
    "    if row[\"text\"] == \"\":\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ï¿½\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"à\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"¥\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ð\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ñ\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"è\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"å\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ç\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"â\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"º¥\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True) \n",
    "    elif \"ë\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"í\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"£\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ø\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"§\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ù\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ã\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df))\n",
    "merged_df.to_csv(\"updated_merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df[\"text\"]\n",
    "y = merged_df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=4)\n",
    "\n",
    "max_words = 30000\n",
    "tokenizer = Tokenizer(num_words=max_words) \n",
    "tokenizer.fit_on_texts(X) \n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train) \n",
    "X_test = tokenizer.texts_to_sequences(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=280)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 50000 # unique vocab\n",
    "maxlen = 280 # the number of words per data point\n",
    "embedding_size = 32 # the dimensions that each word is converted to\n",
    "\n",
    "#tensorboard\n",
    "name = \"LSTM 3 layers\"\n",
    "tboard_log_dir = os.path.join(\"logs\", name)\n",
    "tensorboard = TensorBoard(log_dir = tboard_log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen)) \n",
    "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fitModel = model.fit(X_train, y_train, epochs = 30, batch_size = 126, validation_data=(X_test, y_test), \n",
    "                     verbose=1, callbacks=[tensorboard]) \n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
