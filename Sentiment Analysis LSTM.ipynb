{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.layers import Conv1D, Flatten, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from keras.layers import Dropout\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Alexis\\Documents\\Data_Scraping\\data\\{}.csv'.format(\"twitter_sentiment_1\"), encoding=\"Latin-1\")\n",
    "df['target'] = df[\"target\"].replace(4, 1) # replace labels of 4 with 1, easier for the computer to compute\n",
    "df.drop([\"ids\",\" flag\",\" user\"],axis=1,inplace=True) # drop unneeded columns\n",
    "df = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_stopwords = ['i', 'me', 'my', 'myself', 'we', \"weve\", \"wev\", 'our', 'ours', 'ourselves',\n",
    "                     'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "                     'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', \n",
    "                     'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "                     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these',\n",
    "                     'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
    "                     'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but',\n",
    "                     'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for','with', \n",
    "                     'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', \n",
    "                     'above','below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
    "                     'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', \n",
    "                     'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "                     'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 's', 't', 'can',\n",
    "                     'will', 'just', 'should', \"should've\", 'now', 'd','ll', 'm', 'o', 're', 've', 'y',\n",
    "                     'ma', \"youre\", \"youve\", \"youll\", \"youd\", \"shes\", \"its\", \"thatll\", \"hes\", \"im\", \"x\",\n",
    "                     \"well\", \"wel\", \"w\", \"i'm\", \"u\", \"b\", \"theyre\", \"they're\", \"ms\", \"mrs\", \"mr\", \"s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def cleaning(data):\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    text_df = data[\" text\"]\n",
    "    cleaned_tweets_list = []  \n",
    "    for text in text_df:\n",
    "        html_decoded = BeautifulSoup(text, \"lxml\")# html decoding\n",
    "        html_decoded = html_decoded.get_text() # gets the text from the decoded html\n",
    "        \n",
    "        try: \n",
    "            html_decoded = html_decoded.decode(\"utf-8-sig\") # decoded using utf-8-sig\n",
    "            no_utf = html_decoded.replace(u\"\\ufffd\", \" \") # removes UTF-8 BOM\n",
    "        except: \n",
    "            no_utf = html_decoded\n",
    "            \n",
    "        no_utf = no_utf.encode('utf8').decode('utf8')\n",
    "        string_no_tags = re.sub(r\"@[A-Za-z0-9-_]+\",\" \", no_utf) # removes tags\n",
    "        \n",
    "        no_urls = re.sub(r\"http\\S+\", \" \", string_no_tags) # remove urls\n",
    "       \n",
    "        no_numbers = re.sub(r\"\\d+\", \" \", no_urls) # remove numbers \n",
    "        \n",
    "        no_hashtags = re.sub(r\"#[A-Za-z0-9]+\",\" \",no_numbers) # removes hashtags and anything following it\n",
    "        no_hashtags = [word.lower() for word in no_hashtags.split()]\n",
    "    \n",
    "        removed_stopwords = [word for word in no_hashtags if word not in updated_stopwords]\n",
    "        \n",
    "        stem = \"\"\n",
    "        for words in removed_stopwords:\n",
    "            stem += \"{} \".format(porter.stem(words))\n",
    "            \n",
    "        duplicates_deleted = re.sub('\\s+', ' ', stem).strip() # all spaces are converted to a single spac\n",
    "  \n",
    "        # removes duplicate characters\n",
    "        duplicates_deleted = \"\"\n",
    "        check = \"\"\n",
    "        counter = 0\n",
    "        for word in removed_stopwords:\n",
    "            for character in word:\n",
    "                counter += 1\n",
    "                if character != check:\n",
    "                    check = character\n",
    "                    duplicates_deleted += \"{}\".format(character)\n",
    "                else:\n",
    "                    continue \n",
    "            if len(word) == counter: \n",
    "                duplicates_deleted += \"{}\".format(\" \")\n",
    "                counter = 0\n",
    "                check = \"\"\n",
    "        \n",
    "        duplicates_deleted = duplicates_deleted.replace(\".\", \" \")\n",
    "            \n",
    "        no_punctuation = \"\".join([char.lower() for char in duplicates_deleted if char not in string.punctuation]) \n",
    "        tweets = no_punctuation.lower() # turn everything lowercase\n",
    "\n",
    "        single_spaced = re.sub('\\s+', ' ', no_punctuation).strip()\n",
    "        single_spaced = single_spaced.split(\" \")\n",
    "    \n",
    "        second_removed_stopwords = [word for word in single_spaced if word not in updated_stopwords]\n",
    "\n",
    "        cleaned_tweets = \"\"\n",
    "        for words in second_removed_stopwords:\n",
    "            cleaned_tweets += \"{} \".format(porter.stem(words))\n",
    "        \n",
    "        cleaned_tweets_list.append(cleaned_tweets) # add each list to tweet\n",
    "    \n",
    "    dict_cleaned = {\"text\": cleaned_tweets_list}  # create dictionary for dataframe structure\n",
    "    cleaned_dataframe = pd.DataFrame(dict_cleaned) # creates the dataframe\n",
    "    return cleaned_dataframe\n",
    "\n",
    "cleaned_df = cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(cleaned_df, old_df):\n",
    "    targetList = []\n",
    "    dateList = []\n",
    "    textList = []\n",
    "    for f in old_df[\" date\"]:\n",
    "        dateList.append(f)\n",
    "    for f in old_df[\"target\"]:\n",
    "        targetList.append(f)\n",
    "    for f in cleaned_df[\"text\"]:\n",
    "        textList.append(f)\n",
    "    \n",
    "    merged_df = pd.DataFrame({\"date\": dateList, \"target\": targetList,\"text\": textList})\n",
    "    return merged_df   \n",
    "\n",
    "merged_df = combine(cleaned_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in merged_df.iterrows():\n",
    "    if row[\"text\"] == \"\":\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ï¿½\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"à\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"¥\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ð\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ñ\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"è\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"å\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ç\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"â\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"º¥\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True) \n",
    "    elif \"ë\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"í\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"£\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ø\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"§\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ù\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)\n",
    "    elif \"ã\" in row[\"text\"]:\n",
    "        merged_df.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_df))\n",
    "merged_df.to_csv(\"updated_merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df[\"text\"]\n",
    "y = merged_df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=4)\n",
    "\n",
    "max_words = 400000\n",
    "tokenizer = Tokenizer(num_words=max_words) \n",
    "tokenizer.fit_on_texts(X) \n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % (len(word_index)))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train) \n",
    "X_test = tokenizer.texts_to_sequences(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=280)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 400000 # unique vocab\n",
    "maxlen = 280 # the number of words per data point\n",
    "embedding_size = 32 # the dimensions that each word is converted to\n",
    "\n",
    "#tensorboard\n",
    "name = \"LSTM 3 layers\"\n",
    "tboard_log_dir = os.path.join(\"logs\", name)\n",
    "tensorboard = TensorBoard(log_dir = tboard_log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen)) \n",
    "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fitModel = model.fit(X_train, y_train, epochs = 3, batch_size = 126, validation_data=(X_test, y_test), \n",
    "                     verbose=1, callbacks=[tensorboard]) \n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
